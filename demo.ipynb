{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZI-7IKtJYIIf"},"source":["\n","# ðŸ•ºðŸ•ºðŸ•º Follow Your Pose ðŸ’ƒðŸ’ƒðŸ’ƒ: \n","# Pose-Guided Text-to-Video Generation using Pose-Free Videos"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680855034941,"user":{"displayName":"yue ma","userId":"10565880864345925493"},"user_tz":-480},"id":"GPBJ9ZPNWofP","outputId":"08c669fd-6983-48ab-805f-31878714772c","vscode":{"languageId":"polyglot-notebook"}},"outputs":[],"source":["#@markdown Check type of GPU and VRAM available.\n","!nvidia-smi - -query-gpu = name, memory.total, memory.free - -format = csv, noheader\n","#make sure you are using Tesla T4, 15360 MiB, 15101 MiB\n"]},{"cell_type":"markdown","metadata":{"id":"JGTUagmtwwxo"},"source":["# ðŸ•ºðŸ•ºðŸ•º Install Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21760,"status":"ok","timestamp":1680855062851,"user":{"displayName":"yue ma","userId":"10565880864345925493"},"user_tz":-480},"id":"JmWCIBSnZP4l","outputId":"7cbd0308-8e11-4140-8bff-58d9368d4a13","vscode":{"languageId":"polyglot-notebook"}},"outputs":[],"source":["# @title  Environment Setup\n","!apt-get update\n","!apt install software-properties-common\n","!sudo dpkg --remove --force-remove-reinstreq python3-pip python3-setuptools python3-wheel\n","!apt-get install python3-pip\n","\n","!git clone https: // github.com/jiaxinlarhh/ict3104-team13-2023.git\n","\n","!git clone https: // github.com/open-mmlab/mmpose.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"polyglot-notebook"}},"outputs":[],"source":["# @title  Setup FollowYourPose & MMPose\n","\n","# FollowYourPose\n","!cd / content/ict3104-team13-2023\n","!export PYTHONPATH = /content/ict3104-team13-2023: $PYTHONPATH\n","!python - m pip install - q - U - -pre triton\n","!apt update\n","!python - m pip install - q diffusers == 0.11.1 torch == 1.13.1 transformers == 4.26.0 bitsandbytes == 0.35.4 imageio-ffmpeg xformers == 0.0.16 - -extra-index-url https: // download.pytorch.org/whl/cu113\n","\n","# MMPose\n","%cd / content/mmpose\n","!python3 - m pip install torch torchvision torchaudio - -index-url https: // download.pytorch.org/whl/cu118\n","# install MMEngine, MMCV and MMDetection using MIM\n","!python3 - m pip install - U openmim\n","!mim install mmengine\n","!mim install \"mmcv>=2.0.0\"\n","!mim install \"mmdet>=3.0.0\"\n"]},{"cell_type":"markdown","metadata":{},"source":["# ðŸ•ºðŸ•ºðŸ•º Data Exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"polyglot-notebook"}},"outputs":[],"source":["#@title US T13-3 Input files subfolders\n","\n","import os\n","\n","main_folder = '/content/ict3104-team13-2023/data_folder'\n","\n","if not os.path.exists(main_folder):\n","    os.mkdir(main_folder)\n","\n","subfolders = ['stickman', 'others']\n","\n","for subfolder in subfolders:\n","    subfolder_path = os.path.join(main_folder, subfolder)\n","\n","    if not os.path.exists(subfolder_path):\n","        os.mkdir(subfolder_path)\n","        print(f\"Created subfolder '{subfolder_path}'\")"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"polyglot-notebook"}},"outputs":[],"source":["#@title  US T13-4 Load Video\n","import ipywidgets as widgets\n","import os, cv2\n","from os import listdir\n","from google.colab.patches import cv2_imshow\n","from IPython.display import  HTML\n","from base64 import b64encode\n","\n","# variables\n","data_url = None\n","vid_directory = \"./ict3104-team13-2023/videos\"\n","vid_list = []\n","\n","# store video names in list\n","for files in os.listdir(vid_directory):\n","  if files[0] != \".\":\n","    vid_list.append(files)\n","\n","# show vid name in list as dropdown\n","dropdown = widgets.Dropdown(options=vid_list, value=None)\n","\n","# UI\n","button = widgets.Button(description=\"Enter\")\n","output = widgets.Output()\n","display(dropdown, button, output)\n","\n","# UI functions\n","def on_button_clicked(b):\n","    with output:\n","        mp4 = open(vid_directory+'/'+dropdown.value,'rb').read()\n","        data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","        if mp4 and data_url:\n","          display(HTML(\"\"\"\n","                  <video controls>\n","                        <source src=\"%s\" type=\"video/mp4\">\n","                  </video>\n","                  \"\"\" % data_url))\n","        else:\n","          print(\"error opening vid file\")\n","\n","\n","button.on_click(on_button_clicked)"]},{"cell_type":"markdown","metadata":{"id":"Kbx46ZU6znrs"},"source":["# ðŸ•ºðŸ•ºðŸ•º Inference"]},{"cell_type":"markdown","metadata":{"id":"6GW8-Xz8zwnE"},"source":["Due to memory of GPU, we recommend set video_length=8 in ./config/pose_sample.yaml for running successfully. \n","\n","Meanwhile, we should keep the skeleton frame length(./followyourpose/pipelines/pipeline_followyourpose.py:422 ) equal with video_length"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"polyglot-notebook"}},"outputs":[],"source":["%cd /content/FollowYourPose\n","!pwd\n","!TORCH_DISTRIBUTED_DEBUG=DETAIL accelerate launch txt2video.py --config=\"configs/pose_sample.yaml\"  --skeleton_path=\"./pose_example/vis_ikun_pose2.mov\""]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"polyglot-notebook"}},"outputs":[],"source":["# @title US T13-7 Inference videos with captions\n","\n","from moviepy.editor import VideoFileClip, clips_array\n","import os\n","from moviepy.editor import VideoFileClip\n","from PIL import Image, ImageSequence\n","\n","# combine the inference video (stickman) with the video from charade into a GIF for display later\n","def combine_and_create_gif(mov_path, mp4_path, output_gif_path):\n","    # Loading the vid\n","    mov_video = VideoFileClip(mov_path)\n","    mp4_video = VideoFileClip(mp4_path)\n","\n","    width1, height1 = mov_video.size\n","    width2, height2 = mp4_video.size\n","\n","    min_width = min(width1, width2)\n","    min_height = min(height1, height2)\n","\n","    # Resize both vid to have same width and height\n","    mov_video = mov_video.resize((min_width, min_height))\n","    mp4_video = mp4_video.resize((min_width, min_height))\n","\n","    # Combine the vid side by side\n","    result = clips_array([[mov_video, mp4_video]])\n","\n","    # Write as GIF for display\n","    result.write_gif(output_gif_path, fps=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["# @title US T13-28 Select skeleton video and perform inference\n","import os\n","from ipywidgets import interact, widgets\n","from IPython.display import display, HTML, Image, clear_output\n","from functools import partial\n","import subprocess\n","\n","\n","\n","# Declare file_dropdown as a global variable\n","file_dropdown = None\n","stickman_directory = '/content/ict3104-team13-2023/data_folder/stickman'\n","input_path = \"\"\n","\n","\n","def display_file_dropdown():\n","    # Define the directory path\n","\n","    # Get a list of files in the directory\n","    files = os.listdir(stickman_directory)\n","\n","    # Create a dropdown widget with the default value set to None\n","    global file_dropdown  # Declare as a global variable\n","    file_dropdown = widgets.Dropdown(\n","        options=[''] + files,\n","        description='Select File:',\n","        disabled=False,\n","        value=None,  # Set the default value to None\n","    )\n","\n","    # Create an output widget to display the selected file\n","    output = widgets.Output()\n","\n","    # Define a function to handle the dropdown selection\n","    def on_file_select(change):\n","        selected_file = change.new\n","        with output:\n","            clear_output()  # Clear the previous output in the output widget\n","            # Display the selected MP4 file\n","            if selected_file:\n","                print(f\"Selected File: {selected_file}\")\n","            else:\n","                print(\"No file selected\")\n","\n","    # Attach the function to the dropdown's change event\n","    file_dropdown.observe(on_file_select, names='value')\n","\n","    # Display the dropdown and the output widget\n","    display(file_dropdown)\n","    display(output)\n","\n","def on_button_click(b):\n","    # Perform an action when the button is clicked\n","    selected_file = file_dropdown.value\n","    if selected_file:\n","        global input_path\n","        input_path = os.path.join(stickman_directory, selected_file)\n","        !TORCH_DISTRIBUTED_DEBUG=DETAIL accelerate launch txt2video.py --config=\"configs/pose_sample.yaml\"  --skeleton_path=\"{input_path}\"\n","\n","\n","\n","def user_input():\n","    # Call the function to display the dropdown\n","    display_file_dropdown()\n","\n","    # Create a button widget\n","    button = widgets.Button(description=\"Confirm\")\n","    button.on_click(on_button_click)\n","\n","    # Display the button\n","    display(button)\n","\n","# Call the user_input function\n","user_input()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["from IPython.display import HTML, display, Image\n","import yaml\n","\n","def display_inference_result(result_path, caption):\n","    # Clear the output\n","    clear_output()\n","\n","    # Check if the GIF file exists in the specified directory\n","    if os.path.isfile(result_path):\n","        # Create an HTML div element for centering\n","        center_div = HTML('<div style=\"display: flex; justify-content: center;\">')\n","\n","        # Display the GIF and the caption inside the centered div\n","        display(center_div)\n","        display(Image(filename=result_path))\n","        display(HTML(f'<p style=\"text-align:center; font-size:16px;\">{caption}</p>'))\n","    else:\n","        print(\"GIF file not found in the specified directory.\")\n","\n","# Retrieve the save_path from the file\n","def get_inference_result():\n","  with open(\"/content/ict3104-team13-2023/checkpoints/inference/save_path.txt\", \"r\") as f:\n","    save_path = f.read()\n","  combine_and_create_gif(input_path, save_path)\n","\n","def get_prompt_from_yaml():\n","  # Load the configuration from the YAML file\n","  config_path = \"/content/ict3104-team13-2023/configs/pose_sample.yaml\"  \n","  with open(config_path, \"r\") as config_file:\n","      config = yaml.load(config_file, Loader=yaml.FullLoader)\n","\n","  # Get the prompt string from the configuration\n","  prompts = config[\"validation_data\"][\"prompts\"]\n","\n","  # Assuming there's only one prompt in the list, you can access it as follows\n","  prompt = prompts[0]\n","\n","  # Print the prompt string\n","  return prompt\n","\n","\n","result_path = \"/content/ict3104-team13-2023/data_folder/inference_result/output.gif\"\n","\n","get_inference_result()\n","prompt = get_prompt_from_yaml()\n","display_inference_result(result_path, prompt)"]},{"cell_type":"markdown","metadata":{},"source":["# ðŸ•ºðŸ•ºðŸ•º Training"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"polyglot-notebook"}},"outputs":[],"source":["#@title  User Story T13-8: Select subfolder from dataset folder\n","import ipywidgets as widgets\n","import os, cv2\n","from os import listdir\n","from ipywidgets import Dropdown, interact\n","\n","data_directory = \"./dataset_folder\"\n","directory_dict = {}\n","\n","# get subfolders as key and list of files as value in dict\n","for root, subfolders, files in os.walk(data_directory):\n","    # Skip the root directory itself\n","    if root == data_directory:\n","        continue\n","\n","    subfolder_name = os.path.relpath(root, data_directory)\n","\n","    if \".ipynb\" in subfolder_name:\n","      continue\n","    # Create a list of file names in the subfolder\n","    file_names = [file for file in files]\n","    # Add the subfolder and its file names to the dictionary\n","    directory_dict[subfolder_name] = file_names\n","#print(directory_dict)\n","\n","\n","# dropdown UI\n","subfolder_choices = Dropdown(options =directory_dict.keys())\n","subfolder_files = Dropdown()\n","button = widgets.Button(description=\"Select dataset\")\n","\n","\n","@interact(subfolder = subfolder_choices, dataset = subfolder_files)\n","def print_city(subfolder, dataset):\n","    subfolder_files.options = directory_dict[subfolder]\n","\n","# UI\n","display(button)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"polyglot-notebook"}},"outputs":[],"source":["#@title  User Story T13-6\n","import ipywidgets as widgets\n","import os, cv2\n","import subprocess\n","import yaml\n","from os import listdir\n","from google.colab.patches import cv2_imshow\n","from IPython.display import HTML, clear_output\n","from base64 import b64encode\n","\n","\n","# Define and Instantiate variables\n","charades_data_url = None\n","charades_video_path = \"./charades\"\n","charades_video_list = []\n","chosen_charades_video = None\n","\n","# Store names of charades video in a list\n","for file in os.listdir(charades_video_path):\n","  charades_video_list.append(file)\n","\n","# Show input to accept user prompt\n","prompt_input = widgets.Text(\n","    value='',  # Initial value\n","    placeholder='Enter prompt...',  # Placeholder text\n","    description='Prompt Input: ',  # Label for the input\n",")\n","\n","# Add names of charades video as dropdown options\n","charades_videos_dropdown = widgets.Dropdown(options=charades_video_list, value=None)\n","\n","# UI to show after running this cell\n","choose_charades_video_button = widgets.Button(description=\"Choose Video\")\n","chosen_video_output = widgets.Output()\n","\n","# Display all UI\n","display(prompt_input, charades_videos_dropdown, choose_charades_video_button, chosen_video_output)\n","\n","# method to add user's prompt into pose_sample.yaml\n","def insert_prompt_input_into_config(prompt):\n","\n","  # Load the YAML file\n","  with open('./configs/pose_sample.yaml', 'r') as file:\n","      config = yaml.safe_load(file)\n","\n","  # Access the 'prompts' section\n","  config['validation_data']['prompts'] = [prompt]\n","\n","\n","  # Save the modified configuration back to the file\n","  with open('./configs/pose_sample.yaml', 'w') as file:\n","      yaml.dump(config, file, default_flow_style=False)\n","\n","def generate_gif():\n","  # Change directory to /content/ict3104-team13-2023\n","  os.chdir('/content/ict3104-team13-2023')\n","\n","  # Print the current working directory\n","  print(os.getcwd())\n","\n","  # Set the TORCH_DISTRIBUTED_DEBUG environment variable and launch txt2video.py\n","  subprocess.run(['accelerate', 'launch', 'txt2video.py', '--config=configs/pose_sample.yaml', '--skeleton_path=./pose_example/vis_ikun_pose2.mov'])\n","\n","#\n","def set_charades_video_variables(charades_video_name):\n","  if charades_video_name is not None:\n","    # print(\"Have something\")\n","    pass\n","  with chosen_video_output:\n","        charades_mp4 = open(charades_video_path +'/'+ charades_video_name,'rb').read()\n","        charades_data_url = \"data:video/mp4;base64,\" + b64encode(charades_mp4).decode()\n","        if charades_mp4 and charades_data_url:\n","          video_html = f'<video controls><source src=\"{charades_data_url}\" type=\"video/mp4\"></video>'\n","          # Clear previous output\n","          clear_output()\n","          display(HTML(video_html))\n","        else:\n","          print(\"Cannot open chosen charades video\")\n","\n","# OnClick function for 'Choose Video' button\n","def on_choose_charades_video_button_clicked(b):\n","    pass\n","\n","# OnChange function for dropdown\n","def on_charades_videos_dropdown_change(change):\n","    if change['name'] == 'value' and change['new']:\n","        chosen_charades_video = change['new']\n","        selected_option = change['new']\n","        # print(f\"Selected option: {selected_option}\")\n","        set_charades_video_variables(selected_option)\n","\n","# On Prompt Input 'enter' key press\n","def on_prompt_input_enter_pressed(change):\n","      # print(\"Enter pressed with text:\", prompt_input.value)\n","      insert_prompt_input_into_config(prompt_input.value)\n","      generate_gif()\n","\n","# Attach event functions to UI\n","prompt_input.on_submit(on_prompt_input_enter_pressed)\n","charades_videos_dropdown.observe(on_charades_videos_dropdown_change, names='value')\n","choose_charades_video_button.on_click(on_choose_charades_video_button_clicked)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["#@title  User Story T13-10\n","import ipywidgets as widgets\n","from IPython.display import display\n","\n","model_name_input = widgets.Text(\n","    placeholder='Enter Model Name',\n","    description='Model Name:',\n",")\n","\n","enter_button = widgets.Button(\n","    description='Enter',\n","    button_style='primary', \n",")\n","\n","# add to the model when its out\n","def handle_enter_button_click(b):\n","    model_name = model_name_input.value\n","    print(f'Done')\n","    \n","enter_button.on_click(handle_enter_button_click)\n","display(model_name_input, enter_button)"]},{"cell_type":"markdown","metadata":{},"source":["# ðŸ•ºðŸ•ºðŸ•º Testing"]},{"cell_type":"markdown","metadata":{},"source":["# ðŸ•ºðŸ•ºðŸ•º MMPose"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["#@title Environment Setup\n","!update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.8 2\n","!update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.9 1\n","!python --version\n","!apt-get update\n","!apt install software-properties-common\n","!sudo dpkg --remove --force-remove-reinstreq python3-pip python3-setuptools python3-wheel\n","!apt-get install python3-pip\n","\n","%cd /content\n","\n","# forked michael's mmpose because project needed to change some of the mmpose code\n","!git clone https://github.com/micdiary/mmpose.git\n","\n","#MMPose\n","%cd /content/mmpose\n","!python3 -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","# install MMEngine, MMCV and MMDetection using MIM\n","!python3 -m pip install -U openmim\n","!mim install mmengine\n","!mim install \"mmcv>=2.0.0\"\n","!mim install \"mmdet>=3.0.0\"\n","\n","!python3 -m pip install -r requirements.txt\n","!python3 -m pip install -v -e .\n","\n","!python3 -m pip install setuptools==68.1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["#@title Check MMpose\n","\n","%cd /content/mmpose/\n","\n","# Check Pytorch installation\n","import torch, torchvision\n","\n","print('torch version:', torch.__version__, torch.cuda.is_available())\n","print('torchvision version:', torchvision.__version__)\n","\n","# Check MMPose installation\n","import mmpose\n","\n","print('mmpose version:', mmpose.__version__)\n","\n","# Check mmcv installation\n","from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n","\n","print('cuda version:', get_compiling_cuda_version())\n","print('compiler information:', get_compiler_version())"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["#@title Inference with MMPOSE\n","# run inference on ALL videos in the charades folder\n","import os\n","\n","%cd /content/mmpose\n","\n","charades_video_path = \"/content/ict3104-team13-2023/charades/\"\n","\n","# List all the MP4 files in the specified directory\n","mp4_files = [f for f in os.listdir(charades_video_path) if f.endswith('.mp4')]\n","\n","\n","# Iterate through the MP4 files and run the script for each one\n","for mp4_file in mp4_files:\n","    input_path = os.path.join(charades_video_path, mp4_file)\n","    output_folder = f\"/content/ict3104-team13-2023/data_folder/stickman/\"\n","\n","    !python demo/topdown_demo_with_mmdet.py \\\n","    demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py \\\n","    https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \\\n","    configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-256x192.py \\\n","    https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth \\\n","    --input {input_path} \\\n","    --output-root {output_folder}"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["#@title Inference with MMPOSE (1 video)\n","# run inference on 1 video\n","import os\n","\n","%cd /content/mmpose\n","\n","charades_video_path = \"/content/ict3104-team13-2023/charades/52CKM.mp4\"\n","\n","output_folder = f\"/content/ict3104-team13-2023/data_folder/stickman3/\"\n","\n","!python demo/topdown_demo_with_mmdet.py \\\n","demo/mmdetection_cfg/faster_rcnn_r50_fpn_1class.py \\\n","https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \\\n","configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-256x192.py \\\n","https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth \\\n","--input {charades_video_path} \\\n","--output-root {output_folder}"]},{"cell_type":"markdown","metadata":{},"source":["After conducting spam testing, we discovered that the first library we tested was superior to the other three libraries due to their outdated versions. Therefore, we will revert to using the first library, as it provides the desired skeleton result."]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["#@title  US T13-30\n","\n","import os\n","import tempfile\n","from base64 import b64encode\n","import ipywidgets as widgets\n","from moviepy.editor import VideoFileClip\n","from IPython.display import HTML, display, clear_output\n","\n","stickman_directory = '/content/ict3104-team13-2023/data_folder/stickman'\n","\n","output_directory = tempfile.mkdtemp()\n","converted_videos = []\n","\n","for video_file in os.listdir(stickman_directory):\n","    if video_file.endswith('.mp4'):\n","        video_path = os.path.join(stickman_directory, video_file)\n","        output_path = os.path.join(output_directory, os.path.splitext(video_file)[0] + '_h264.mp4')\n","\n","        clip = VideoFileClip(video_path)\n","        clip.write_videofile(output_path, codec='libx264', logger=None)\n","\n","        converted_videos.append(output_path)\n","        print(f\"Done converting {video_file}\")\n","\n","stickman_files = [f for f in os.listdir(output_directory) if f.endswith('_h264.mp4')]\n","\n","stickman_dropdown = widgets.Dropdown(\n","    options=stickman_files,\n","    description='Select Video:'\n",")\n","\n","output = widgets.Output()\n","\n","def display_selected_video(change):\n","    with output:\n","      clear_output()\n","      selected_video = change.new\n","      video_path = os.path.join(output_directory, selected_video)\n","\n","      with open(video_path, 'rb') as f:\n","          data = f.read()\n","          data_url = \"data:video/mp4;base64,\" + b64encode(data).decode()\n","          display(HTML(f\"\"\"\n","              <video controls autoplay>\n","                  <source src=\"{data_url}\" type=\"video/mp4\">\n","              </video>\n","          \"\"\"))\n","\n","stickman_dropdown.observe(display_selected_video, names='value')\n","display(stickman_dropdown)\n","display(output)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOyNNhRT8BBHJeqGAp6TaSp","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
